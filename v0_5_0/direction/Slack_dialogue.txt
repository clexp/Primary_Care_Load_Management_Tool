Hi,
Few questions:
I am not sure where I am trying to get to with this project.  In a sense the answer is obvious: a call workload analysis tool for primary care staffing management, with end goal: gets used in real life by actual users.  So should I be talking with the end user (a person) and saying, "what changes would make you want to use this?"
There's data analysis I do first, which is distinct and separate from the streamlit app.  This is jupyter notebooks have some data assessment, which then informs what we can do in the streamlit app. They are all in the repository.  Does that matter? should it be kept out of the repository?
I've put no work into the 'visual appearance' of the app. That matters as far as .. user friendliness and .. accessibility?
Getting back to target completion, what constitutes 'complete' or 'finished' as far as the HSMA is concerned?
The 'quick insights' section will have more from the jupyter notebooks.  Having seen lots of projects (and Datacamp?), would you be able to comment on what works well here?
There was going to be a Discrete Event Simulation section. This seems 'nice to have' now, as I think the users needs are met without it.  I could add it anyway, taking the call data as parameters, and running the simulation. 
Machine Learning / Prediction.  How would this fit in? Predict the call wait time given past history?  This is affected by staffing level in the past.  I am struggling to get staffing levels from my colleagues in the practice.  Perhaps I should try to predict the incoming call rate, not the wait time as that is not dependent on staff levels?
Up to October, we had one manager who got the call wait right down, but had loads of staff.  From October, another manager came in and there was a lot of staff turn over, the staff level is down but the wait time is up.  Neither of them are sharing the staffing level (have been asked) but this affects the wait time.  Can you advise on encouraging data sharing from colleagues?
Thanks again


Sammi Rosser { HSMA Trainer }
  12:07 PM
Hi Chris - here are some of my thoughts so far. We can expand on any of those when we meet :slightly_smiling_face:
I am not sure where I am trying to get to with this project.  In a sense the answer is obvious: a call workload analysis tool for primary care staffing management, with end goal: gets used in real life by actual users.  So should I be talking with the end user (a person) and saying, "what changes would make you want to use this?"
Having a chat to the end user would be my recommendation. It sounds like your main goal is to reduce call waits, so working with an end user to identify what ‘levers’ they can pull in this instance - which they would hopefully find value in being able to try out in a safe/free environment before investing the time/funds in making the changes in the real world. It might be that they can’t actually get more staff, but maybe they can put them on at different times or stagger shifts differently, or maybe they can change the length limit before callers are cut off, or something else.
I think a tool which does any of the following would be a successful tool:
Prompts discussions about the way things are currently done/gets people talking about possible system improvements beyond the norm
Is used as a tool for people to try out changes and explore their outcome
Leads to the trialling of some suggested changes in the system
2. There's data analysis I do first, which is distinct and separate from the streamlit app.  This is jupyter notebooks have some data assessment, which then informs what we can do in the streamlit app. They are all in the repository.  Does that matter? should it be kept out of the repository?
It depends whether there’s anything in there you are concerned about being available more publicly. If there’s no issue, I think it’s always helpful to keep everything together in the repo, though generally in separate folders (which you’ve already done). In subfolders on github, you can provide additional README.md files that would appear when someone navigates to that folder on github, which can help with making their purpose clearer. But overall, if it’s something that a subsequent user would need to do to be able to use your app, I’d be keen to include it in some way.
You may want to explore something like nbstripout if you want the code to be available but not the outputs for your own system - it’s just an automated way of running the ‘clear all outputs’ option from your notebook prior to upload. https://github.com/kynan/nbstripout
3. I've put no work into the 'visual appearance' of the app. That matters as far as .. user friendliness and .. accessibility?
It’s always a tricky one to know when to put the time into that side of things! As it can be a bit of a sunk cost if you end up making dramatic changes, but I find it does go a long way to building user confidence/trust in the app. A lot of the streamlit inputs have a ‘help’ parameter to add a tooltip, which I find helpful. I also use st.caption quite a lot to build in some more subtle help text/explanations. Extra pages with things like a glossary and a beginner-friendly overview of the technique in the context of the problem you’re solving is worthwhile too.
I also find providing simple, clear outputs really matter to the end users - so things like streamlit metric cards with key KPIs, or interactive plotly charts that look polished and minimize the amount of thinking the user has to do to interpret them, make a lot of difference.
4. Getting back to target completion, what constitutes 'complete' or 'finished' as far as the HSMA is concerned?
In pure terms of certification, we’re looking for evidence of having picked up and applied some of the concepts from the course. I haven’t fully assessed your repository yet but I’d say you’re showing that you’ve done that!
Technically HSMA 6 ends at the end of June - but I would anticipate us continuing to provide support for projects beyond that under the new project structure - in previous rounds there was a much fimer end point but things are a lot more flexible now.
Our ideal on the programme is being able to gather impact stories of models being built and going on to have a positive impact on service users or staff in some way. So I’d say our ultimate ‘complete’ project would be along the lines of ‘this model is built and validated, it’s now embedded in the service and used as part of business as usual for data-driven decision making, and since it’s introduction we saw an x% decrease in long waits for calls/some other valuable KPI or cost saving’.
5. The 'quick insights' section will have more from the jupyter notebooks.  Having seen lots of projects (and Datacamp?), would you be able to comment on what works well here?
The stuff you’ve done there looks great! I think it would just be again thinking about your end user and their level of data literacy, and thinking about what data you can provide that would give them actionable insights. I have generally found people to have very little time to spend understanding a graph, so it either needs to be super simple or to walk them through what the graph is telling them in words - they may not actually be able to draw that insight out themslves. You’ve already started to do this with a lot of your text, so incorporating this and perhaps including a date that shows when you last reviewed the data would be good - as the patterns may be unlikely to change in the short term, but it would be useful to a future user to know whether the interpretation is recent or not.
Things like faceting by variables like day of week can also help to tell the story more easily.
People often seem to like interactive charts (e.g. plotly) as then they can hover over and see the exact numbers - even if that’s not actually that helpful to them. Though plotly can also be a problem for less tech-savvy users as it’s easy to accidentally zoom in and not understand how you’ve done that!
You can always provide additional graphs/outputs inside of expanders/tabs etc. for users who might want to dive deeper (though bare in mind this generally means you have more to maintain, which can be a pain!), but picking out a few key outputs and giving some good explanation of how to interpret the graphs will likely maximise the impact for your general user base.
For terms like ‘standard deviation’, you may want to think about different ways you can describe that for non-specialist users (so just leaning in to how you are showing the variation in calls).
I would love to see some more visualising of the breakdown of the successful calls vs not connected calls and those reasons too. That looks like a really interesting area and having that as a line plot faceted by reason/outcome across the day could be really interesting.
You might also want to look at something like SPC charts (unless this is already provided to them elsewhere). This might help draw out any concerning trends that you could highlight to the user - e.g. if call demand seems to be continuing to grow month on month.
6. There was going to be a Discrete Event Simulation section. This seems 'nice to have' now, as I think the users needs are met without it.  I could add it anyway, taking the call data as parameters, and running the simulation.
So I think there are a couple of considerations here.
If DES is of interest to you and is a tool you can see a use for elsewhere in your work, this looks to me like it would be a really nice practice project for it.
The other side of it is that DES may also cope better with some of the nuances of GP surgery call workload like the 8am rush.
I sometimes find DES is an easier sell to stakeholders because they can get their head around the idea of individual simulated callers with a bit of guidance, whereas a ‘formula’ can be a bit scary and opaque (though I know people’s opinions of which is an easier sell vary quite a lot!).
You could also incorporate things like animations to help people better visualise the buildup of queues at certain times of day, and how much time staff spend free at different times.
DES might also give you more granular control over parts of the system - so you could better model the sharp spike at 8am with time-dependent arrival modelling (https://des.hsma.co.uk/modelling_variable_arrival_rates.html), and play around with things like how patient the callers are before ringing off.
One consideration is that you’d ideally have more granular (call-level) data on call durations to be able to best fit the call duration distributions to - though you could likely work well enough with the average durations you have and an exponential distribution might actually still be a sensible choice.
I attended an operational research conference the other day and there was an interesting talk on validating models by applying two different approaches (one DES, one simpler model), which this reminds me of - so if you created a DES model separately to your Erlang model, and they give similar outputs, then it may increase your confidence in their predictions.
So I think DES could be a really strong addition to the work and would be a great fit too - but it may depend on your goals in the long run.
7. Machine Learning / Prediction.  How would this fit in? Predict the call wait time given past history?  This is affected by staffing level in the past.  I am struggling to get staffing levels from my colleagues in the practice.  Perhaps I should try to predict the incoming call rate, not the wait time as that is not dependent on staff levels?
So you could use something fairly simple like a Prophet forecast to predict likely demand in the future. Prophet is very good with seasonality, so if you, say, see more demand in Winter for appointments, then Prophet is likely to pick that up quite well.
I would take that approach - predict the incoming call rate to potentially feed that into your model, then your DES/Erlang model would give you the predictions for the performance - you want those predictions to emerge from the behaviour of the model as wait will be such a complex function of demand and capacity that predicting it based on historical patterns (as Prophet would do) is not likely to perform well - whereas Prophet can perform well on the demand as long as that’s expected to continue in broadly the same way for the time period of interest (i.e. you’re not making a dramatic change).
You could also automatically run the model under a range of different staffing levels, perhaps, and use this to give the user an indication of the staffing level/pattern they would need to achieve a particular level of performance, based on the predicted demand.
12:07
8. Up to October, we had one manager who got the call wait right down, but had loads of staff.  From October, another manager came in and there was a lot of staff turn over, the staff level is down but the wait time is up.  Neither of them are sharing the staffing level (have been asked) but this affects the wait time.  Can you advise on encouraging data sharing from colleagues?
It’s often a very chicken-and-egg scenario - until they see how it can help them, they won’t share it (or at least won’t prioritise it), but it’s hard to show how it will help without the data!
So I think there are a couple of ways
Getting them more involved in discussions around how it could help them, so they feel like they have some ownership and a stake in it
Showing them some positive experiences from other practices (e.g. https://www.strategyunitwm.nhs.uk/news/bringing-patient-flow-modelling-general-practice)
Making clear it’s not about their previous rotas being ‘right’ or ‘wrong’, and that it’s not going to reflect badly on them - I think there’s often a lot of fear around this, and getting across to people that it’s just an important part of building the system model so that plans can be made going forward, and building up a picture of what has worked well and less well previously can avoid us repeating things.
Alternatively, just putting in a rota and almost baiting them into going ‘that’s entirely wrong, it’s actually…’ (though this approach can backfire!)
The Strategy UnitThe Strategy Unit
Bringing patient flow modelling into general practice
With general practice appointments hitting the highest numbers on record (34.8 million in England alone in November 2021), careful organisation and planning for patient appointments is increasingly important.