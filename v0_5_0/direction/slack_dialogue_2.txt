Chris: Hi,
Sorry to hear Dan has gone off, I hope he is ok.  Sounds like you are running the show for a while, hope that works out for you.
Thank you again for everything you do, I really think HSMA is excellent, and I am nudging some people to enter for round 7 already!!
Sorry about not logging on to see messages for a while, you messaged me about having a look at my work on github.  I did post a link after a while, and added Bergam0t as a collaborator.  I booked 2pm on 7-4-25 with you for advice.  Would it help if I sent a few questions ahead of time?
Many thanks
Chris
===
SR:
Thanks Chris - hopefully keeping the show on the road for now, but looking forward to Dan's return!
I'm delighted to hear that you might have some people lined up for round 7 :smile: It's looking more likely to start in early 2026 now but we've got lots of exciting improvements planned.
Thanks for adding me as a collaborator - that seems to have worked, but I haven't managed to make time to take a proper look yet. Will have a skim ahead of our catch-up, but a few questions ahead of time would indeed be extremely useful.
Cheers!
===
Chris:
Hi, I have done some updates, I will push them to the streamlit site shortly.  I have a few questions for tomorrow's meeting, do you want them here first? or emailed or wait for tomorrow?
Chris
===
SR:
Hi Chris - if you're able to send questions over on here today, that would be fab - ta.
===
Chris:
Hi,
Few questions:
I am not sure where I am trying to get to with this project.  In a sense the answer is obvious: a call workload analysis tool for primary care staffing management, with end goal: gets used in real life by actual users.  So should I be talking with the end user (a person) and saying, "what changes would make you want to use this?"
There's data analysis I do first, which is distinct and separate from the streamlit app.  This is jupyter notebooks have some data assessment, which then informs what we can do in the streamlit app. They are all in the repository.  Does that matter? should it be kept out of the repository?
I've put no work into the 'visual appearance' of the app. That matters as far as .. user friendliness and .. accessibility?
Getting back to target completion, what constitutes 'complete' or 'finished' as far as the HSMA is concerned?
The 'quick insights' section will have more from the jupyter notebooks.  Having seen lots of projects (and Datacamp?), would you be able to comment on what works well here?
There was going to be a Discrete Event Simulation section. This seems 'nice to have' now, as I think the users needs are met without it.  I could add it anyway, taking the call data as parameters, and running the simulation. 
Machine Learning / Prediction.  How would this fit in? Predict the call wait time given past history?  This is affected by staffing level in the past.  I am struggling to get staffing levels from my colleagues in the practice.  Perhaps I should try to predict the incoming call rate, not the wait time as that is not dependent on staff levels?
Up to October, we had one manager who got the call wait right down, but had loads of staff.  From October, another manager came in and there was a lot of staff turn over, the staff level is down but the wait time is up.  Neither of them are sharing the staffing level (have been asked) but this affects the wait time.  Can you advise on encouraging data sharing from colleagues?
Thanks again
===
SR:

Sammi Rosser { HSMA Trainer }
:computer:  12:07 PM
Hi Chris - here are some of my thoughts so far. We can expand on any of those when we meet :slightly_smiling_face:
I am not sure where I am trying to get to with this project.  In a sense the answer is obvious: a call workload analysis tool for primary care staffing management, with end goal: gets used in real life by actual users.  So should I be talking with the end user (a person) and saying, "what changes would make you want to use this?"
Having a chat to the end user would be my recommendation. It sounds like your main goal is to reduce call waits, so working with an end user to identify what ‘levers’ they can pull in this instance - which they would hopefully find value in being able to try out in a safe/free environment before investing the time/funds in making the changes in the real world. It might be that they can’t actually get more staff, but maybe they can put them on at different times or stagger shifts differently, or maybe they can change the length limit before callers are cut off, or something else.
I think a tool which does any of the following would be a successful tool:
Prompts discussions about the way things are currently done/gets people talking about possible system improvements beyond the norm
Is used as a tool for people to try out changes and explore their outcome
Leads to the trialling of some suggested changes in the system
2. There's data analysis I do first, which is distinct and separate from the streamlit app.  This is jupyter notebooks have some data assessment, which then informs what we can do in the streamlit app. They are all in the repository.  Does that matter? should it be kept out of the repository?
It depends whether there’s anything in there you are concerned about being available more publicly. If there’s no issue, I think it’s always helpful to keep everything together in the repo, though generally in separate folders (which you’ve already done). In subfolders on github, you can provide additional README.md files that would appear when someone navigates to that folder on github, which can help with making their purpose clearer. But overall, if it’s something that a subsequent user would need to do to be able to use your app, I’d be keen to include it in some way.
You may want to explore something like nbstripout if you want the code to be available but not the outputs for your own system - it’s just an automated way of running the ‘clear all outputs’ option from your notebook prior to upload. https://github.com/kynan/nbstripout
3. I've put no work into the 'visual appearance' of the app. That matters as far as .. user friendliness and .. accessibility?
It’s always a tricky one to know when to put the time into that side of things! As it can be a bit of a sunk cost if you end up making dramatic changes, but I find it does go a long way to building user confidence/trust in the app. A lot of the streamlit inputs have a ‘help’ parameter to add a tooltip, which I find helpful. I also use st.caption quite a lot to build in some more subtle help text/explanations. Extra pages with things like a glossary and a beginner-friendly overview of the technique in the context of the problem you’re solving is worthwhile too.
I also find providing simple, clear outputs really matter to the end users - so things like streamlit metric cards with key KPIs, or interactive plotly charts that look polished and minimize the amount of thinking the user has to do to interpret them, make a lot of difference.
4. Getting back to target completion, what constitutes 'complete' or 'finished' as far as the HSMA is concerned?
In pure terms of certification, we’re looking for evidence of having picked up and applied some of the concepts from the course. I haven’t fully assessed your repository yet but I’d say you’re showing that you’ve done that!
Technically HSMA 6 ends at the end of June - but I would anticipate us continuing to provide support for projects beyond that under the new project structure - in previous rounds there was a much fimer end point but things are a lot more flexible now.
Our ideal on the programme is being able to gather impact stories of models being built and going on to have a positive impact on service users or staff in some way. So I’d say our ultimate ‘complete’ project would be along the lines of ‘this model is built and validated, it’s now embedded in the service and used as part of business as usual for data-driven decision making, and since it’s introduction we saw an x% decrease in long waits for calls/some other valuable KPI or cost saving’.
5. The 'quick insights' section will have more from the jupyter notebooks.  Having seen lots of projects (and Datacamp?), would you be able to comment on what works well here?
The stuff you’ve done there looks great! I think it would just be again thinking about your end user and their level of data literacy, and thinking about what data you can provide that would give them actionable insights. I have generally found people to have very little time to spend understanding a graph, so it either needs to be super simple or to walk them through what the graph is telling them in words - they may not actually be able to draw that insight out themslves. You’ve already started to do this with a lot of your text, so incorporating this and perhaps including a date that shows when you last reviewed the data would be good - as the patterns may be unlikely to change in the short term, but it would be useful to a future user to know whether the interpretation is recent or not.
Things like faceting by variables like day of week can also help to tell the story more easily.
People often seem to like interactive charts (e.g. plotly) as then they can hover over and see the exact numbers - even if that’s not actually that helpful to them. Though plotly can also be a problem for less tech-savvy users as it’s easy to accidentally zoom in and not understand how you’ve done that!
You can always provide additional graphs/outputs inside of expanders/tabs etc. for users who might want to dive deeper (though bare in mind this generally means you have more to maintain, which can be a pain!), but picking out a few key outputs and giving some good explanation of how to interpret the graphs will likely maximise the impact for your general user base.
For terms like ‘standard deviation’, you may want to think about different ways you can describe that for non-specialist users (so just leaning in to how you are showing the variation in calls).
I would love to see some more visualising of the breakdown of the successful calls vs not connected calls and those reasons too. That looks like a really interesting area and having that as a line plot faceted by reason/outcome across the day could be really interesting.
You might also want to look at something like SPC charts (unless this is already provided to them elsewhere). This might help draw out any concerning trends that you could highlight to the user - e.g. if call demand seems to be continuing to grow month on month.
6. There was going to be a Discrete Event Simulation section. This seems 'nice to have' now, as I think the users needs are met without it.  I could add it anyway, taking the call data as parameters, and running the simulation.
So I think there are a couple of considerations here.
If DES is of interest to you and is a tool you can see a use for elsewhere in your work, this looks to me like it would be a really nice practice project for it.
The other side of it is that DES may also cope better with some of the nuances of GP surgery call workload like the 8am rush.
I sometimes find DES is an easier sell to stakeholders because they can get their head around the idea of individual simulated callers with a bit of guidance, whereas a ‘formula’ can be a bit scary and opaque (though I know people’s opinions of which is an easier sell vary quite a lot!).
You could also incorporate things like animations to help people better visualise the buildup of queues at certain times of day, and how much time staff spend free at different times.
DES might also give you more granular control over parts of the system - so you could better model the sharp spike at 8am with time-dependent arrival modelling (https://des.hsma.co.uk/modelling_variable_arrival_rates.html), and play around with things like how patient the callers are before ringing off.
One consideration is that you’d ideally have more granular (call-level) data on call durations to be able to best fit the call duration distributions to - though you could likely work well enough with the average durations you have and an exponential distribution might actually still be a sensible choice.
I attended an operational research conference the other day and there was an interesting talk on validating models by applying two different approaches (one DES, one simpler model), which this reminds me of - so if you created a DES model separately to your Erlang model, and they give similar outputs, then it may increase your confidence in their predictions.
So I think DES could be a really strong addition to the work and would be a great fit too - but it may depend on your goals in the long run.
7. Machine Learning / Prediction.  How would this fit in? Predict the call wait time given past history?  This is affected by staffing level in the past.  I am struggling to get staffing levels from my colleagues in the practice.  Perhaps I should try to predict the incoming call rate, not the wait time as that is not dependent on staff levels?
So you could use something fairly simple like a Prophet forecast to predict likely demand in the future. Prophet is very good with seasonality, so if you, say, see more demand in Winter for appointments, then Prophet is likely to pick that up quite well.
I would take that approach - predict the incoming call rate to potentially feed that into your model, then your DES/Erlang model would give you the predictions for the performance - you want those predictions to emerge from the behaviour of the model as wait will be such a complex function of demand and capacity that predicting it based on historical patterns (as Prophet would do) is not likely to perform well - whereas Prophet can perform well on the demand as long as that’s expected to continue in broadly the same way for the time period of interest (i.e. you’re not making a dramatic change).
You could also automatically run the model under a range of different staffing levels, perhaps, and use this to give the user an indication of the staffing level/pattern they would need to achieve a particular level of performance, based on the predicted demand.
12:07
8. Up to October, we had one manager who got the call wait right down, but had loads of staff.  From October, another manager came in and there was a lot of staff turn over, the staff level is down but the wait time is up.  Neither of them are sharing the staffing level (have been asked) but this affects the wait time.  Can you advise on encouraging data sharing from colleagues?
It’s often a very chicken-and-egg scenario - until they see how it can help them, they won’t share it (or at least won’t prioritise it), but it’s hard to show how it will help without the data!
So I think there are a couple of ways
Getting them more involved in discussions around how it could help them, so they feel like they have some ownership and a stake in it
Showing them some positive experiences from other practices (e.g. https://www.strategyunitwm.nhs.uk/news/bringing-patient-flow-modelling-general-practice)
Making clear it’s not about their previous rotas being ‘right’ or ‘wrong’, and that it’s not going to reflect badly on them - I think there’s often a lot of fear around this, and getting across to people that it’s just an important part of building the system model so that plans can be made going forward, and building up a picture of what has worked well and less well previously can avoid us repeating things.
Alternatively, just putting in a rota and almost baiting them into going ‘that’s entirely wrong, it’s actually…’ (though this approach can backfire!)
The Strategy UnitThe Strategy Unit
Bringing patient flow modelling into general practice
With general practice appointments hitting the highest numbers on record (34.8 million in England alone in November 2021), careful organisation and planning for patient appointments is increasingly important.
===
Chris: Thanks for your time today
===
SR:
No problem at all, Chris! Was great to have a chat about your project. I ended up getting caught up with some other bits this afternoon but will get a full follow-up message sent to you tomorrow :+1:
===
Chris:
If I can summarise, we talked about the following
Adding in discrete event simulation
Going back to the original data and predict just the call volume, not the waiting time, to isolate the staffing levels
Predicting the call volume (which ML tool?) and feeding this into the discreet event simulation
Using the output of discrete event simulation through an erlang model to state staff requirements
Parallel and alongside this, use the staffing level data against the wait time data to develop a new parameter which we could name 'call completion pressure', this is likely to depend on the queue length and the waiting time though we may need to do a 'principal cause analysis'. (New territory for me).
Use the 'phone call completion pressure' as a model to predict the staff requirements, this would be parallel to the erlang model, and probably use an ML tool (which?).
Approach the Practice manager for broader data about queue drop out / hangup times. At the moment we think we only have callers that do hang up, not callers which do not hang up.
When we get said data use the python 'prophet' library to add in seasonal variation to the hangup time data, as a third prediction model, (Feed to DES and/or other wait time prediction model).
Keep the 'Insights' simple. 
Not using agent based simulation yet. 
Aim to get user buy in to the tool. 
What would you add / remove / alter?
Many thanks
===
SR:
Thank you Chris! Sorry for the further delay on that.
Here's my proposed summary of next steps:
Adding in discrete event simulation
I'd kick off by sketching out a model of the system - where are the points patients could leave or reenter the system, for example? What resources (your call handlers) exist at different stages? You might find this chapter of the little book of DES to be a good recap. https://des.hsma.co.uk/intro_to_des_concepts.html
I'd then build a simple first version of the model. I personally usually start by just making it so patients arrive, are seen, and then leave the system (something a bit like our simplest example here, but you'd be changing out nurses for call handlers).
https://des.hsma.co.uk/an_example_simpy_model.html
There's also an example of a model from the lecture (https://des.hsma.co.uk/exercise_gp.html) where it's modelling a simple GP surgery - though you wouldn't need all of the steps here as you're only interested in the capacity to answer calls, not what happens afterwards. However, that might give you an idea of how you might model other aspects of capacity - like receptionists handling people arriving in person, though parameterising that part of your model may be tricker!
https://github.com/hsma-programme/h6_simpy_part_1/blob/main/2b_simpy_part_1/solutions/exercise_1_solution.py
Then you can start to add in the more complex elements.
some people calling back if they are unsuccessful the first time
modelling the 8am rush, plus seasonality throughout the week: https://des.hsma.co.uk/modelling_variable_arrival_rates.html
queueing behaviours like reneging (people hanging up because they've reached the maximum time they are willing/able to wait) and balking (people not joining the queue because of that 30 caller queue limit): https://des.hsma.co.uk/reneging_balking_jockeying.html
varying the number of call handlers available at different times of day (https://des.hsma.co.uk/modelling_resource_unavailability.html)
using real-world distributions that accurately match the call times you've seen
Throughout this, think about the elements you want to track and summarize (like the number of people who renege/balk, the average waits for answered calls/all calls, etc.)
---
Using the output of discrete event simulation through an erlang model to state staff requirements
So rather than feeding it into the Erlang model, I'd look at using the discrete event simulation model itself to predict the staffing requirements. By varying the number of resources - call handlers, while holding the number of incoming calls consistent, you can start to predict the required
---
Going back to the original data and predict just the call volume, not the waiting time, to isolate the staffing levels
Predicting the call volume (which ML tool?) and feeding this into the discreet event simulation
When working with a discrete event simulation model, you want the actual behaviour to emerge from the properties of the model, not be predetermined by the metrics you're actually trying to measure - so feeding in the call volumes is the way to go. You then track the wait times that are experiences by the entities (callers) in the model, varying the resource (call handlers) available, with call lengths being variable but based on historically observed patterns too.
In terms of prediction, I'd recommend the Prophet library. Prophet is good for short to medium term prediction in cases where you are expecting the patterns of demand to remain broadly similar. In its simplest guise, it is only basing its predictions on the historical data fed to it - so i.e. it only has the historical number of calls available to predict the future number of calls, as opposed to factoring in loads of other things like demand on other parts of the system, weather, flu patterns, etc. - but it's likely to be 'good enough' for this approach.
Of the time-series models out there, Prophet is user-friendly and also copes well with strongly seasonal data. It can be aware of holidays too, so should cope well with spikes/lulls due to things like bank holidays.
However, you may not need to be predicting future call volumes via Prophet to start making use of the model - if historical demand is expected to be fairly representative still at this point, you could use that, and perhaps look to build Prophet into the process further down the line
---
Approach the Practice manager for broader data about queue drop out / hangup times. At the moment we think we only have callers that do hang up, not callers which do not hang up.
Yep - it's important to be clear on exactly what is held in that dataset. You could start building your model without having this data to hand - in HSMA we generally recommend starting off with an Exponential distribution (that's what random.expovariateis in the example code), and then you can work on parameterising distributions with the real data further down the line.